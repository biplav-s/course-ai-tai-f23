{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoU5JTRyk-6m"
      },
      "source": [
        "# Regret Matching and Minimisation\n",
        "\n",
        "##Learning Objectives\n",
        "\n",
        "*   Learn about regret matching.\n",
        "*   Understand how self-play regret matching can learn Nash Equilibria.\n",
        "*   Write a regret matching algorithm to solve Rock, Paper, Scissors.\n",
        "\n",
        "The regret matching algorithm that you learn here is a form of machine learning that can be used to solve games for Nash Equilibrium [1]. \n",
        "\n",
        "We recommend that you have some basic python programming experience.  \n",
        "\n",
        "## Regret Minimisation\n",
        "\n",
        "Regret is defined as how much you would have preferred to have played a different strategy than the one that you did play. By minimising regret, we can find Nash Equilibria in zero-sum normal form games. There are a range of regret minimisation algorithms. We will be using regret matching, which is one of the simpler algorithms.\n",
        "\n",
        "Rock Paper Scissors is used as a simple game with a known solution in order to learn about the algorithm. Regret matching can be used to solve harder games, and it is a key part of Counter Factual Regret Minimisation algorithms, which can solve games with incomplete information such as Texas Hold Em poker [2].\n",
        "\n",
        "## This Notebook\n",
        "\n",
        "This is a Jupyter notebook, that lets you run blocks of code interactively. To run some code, simply click on the code block and click on the play icon that will appear in the top left of the code block.\n",
        "\n",
        "## Coding Approach\n",
        "\n",
        "During the notebook you will write your own regret matching algorithm. We will guide you through step by step and we have provided the basic structure of the functions you will need. Your job is to fill in the code in the functions. In many of these, we have written tests below the functions as 'assert' statements. You can use these to check that your code works correctly - if the assert statement raises an error then you need to change your code. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zlpLGN0cHOM"
      },
      "source": [
        "# Rock Paper Scissors\n",
        "Rock Paper Scissors, RPS, or Roshambo, is a well known game. It may seem random but there are competitive tournaments.\n",
        "\n",
        "Play the code below to see a competitive RPS game for a $50,000 prize."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsPOmn7Wk9n8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "02f05f45-7189-40eb-9978-375e8e51ddc4"
      },
      "source": [
        "from IPython.display import IFrame\n",
        "\n",
        "IFrame(\"https://www.youtube.com/embed/6yrdT5y12kA\", width=560, height=315)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7fd4021b1ad0>"
            ],
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"560\"\n",
              "            height=\"315\"\n",
              "            src=\"https://www.youtube.com/embed/6yrdT5y12kA\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIL1XKA0daoV"
      },
      "source": [
        "## Game Theory Definitions\n",
        "\n",
        "Rock Paper Scissors can be modelled as a 2 player simultaneous zero-sum Normal Form\n",
        "game. 2 players choose an action at the same time, such that the payoffs always sum to zero i.e. if I win some utility then my opponent loses the same amount of utility. \n",
        "\n",
        "A Normal Form game is defined as a tuple $(N, S, A, u)$.\n",
        "\n",
        "*   $N = \\{1,2\\}$ is the set of players.\n",
        "*   $S_i = \\{Rock, Paper, Scissors\\}$ is the set of actions for each player.\n",
        "*   $A = S_1 \\times S_2$ is the set of possible simultaneous actions by both players. \n",
        "*   $u$ is a function mapping each action profile to a utility for each player. \n",
        "\n",
        "The game can be represented in a payoff matrix, such that each entry is $(u_1, u_2)$.\n",
        "\n",
        "\\begin{array}{ccc}\n",
        "&\\text{Rock}&\\text{Paper}&\\text{Scissors}\\\\\n",
        "\\text{Rock} & (\\;0,\\;0) & (-1,1) & (1,-1)\\\\\n",
        "\\text{Paper} & (1,-1) & (\\; 0,\\;0) & (-1,1)\\\\\n",
        "\\text{Scissors} &(-1,1) & (1,-1) & (\\; 0,\\;0)\n",
        "\\end{array}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igsHQj9Lgl4w"
      },
      "source": [
        "## Regret Matching\n",
        "\n",
        "### Utility\n",
        "Imagine that we are playing Rock Paper Scissors for money. Whoever loses the game gives £1 to the winner. If we play Rock and our opponent plays Scissors then we win £1, if our opponent plays Paper then we lose £1 and if our opponent plays Rock it is a tie and we do not gain or lose any money. We can describe our winnings as a utility, either 1,0 or -1. \n",
        "\n",
        "### Regret\n",
        "Let us say that in the first round we play Rock and our opponent plays Paper - unfortunately we lose £1.\n",
        "\n",
        "*   We do not have any regret of not playing Rock, because we did play Rock. So we can set our regret here to 0. \n",
        "*   We regret not having played Paper and drawing, in which case we would have not lost or gained any money, so we would have been £1 better off than what actually happened. So we can say that the value of our regret of not playing Paper is £1.\n",
        "*   We also regret not having played Scissors and winning £1, which would have left us £2 better off than what we did play. So our regret of not playing Scissors is £2.  \n",
        "\n",
        "So after playing Rock against our opponent's Paper, our **total regrets** for the actions (Rock, Paper, Scissors) can be written as (0,1,2). Formally, regret is the utility of the action we could have played minus the utility of the action that we did play.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHeBRbDboPEB"
      },
      "source": [
        "## Challenge - Write A Simple Function That Gives The Utility for Player 1 of Each Action, If We Know Player 2's Action\n",
        "\n",
        "If we know player 2's action, what are the payoffs for Rock, Paper and Scissors for player 1? \n",
        "\n",
        "We can assign each action a number. So Rock is 0, Paper is 1 and Scissors is 2. \n",
        "\n",
        "Try and write the function below. The payoff matrix given above might be useful. We've also written a few tests so you can see if your function is correct or not. Read the function docstring (the comments) for more information about how the function should behave."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOXPPjW80hjb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43d21a1b-4725-4202-978d-1031854d4882"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def get_action_utilities(opponent_action):\n",
        "    \"\"\" \n",
        "    Get the utilities for player 1 of playing an action, \n",
        "    if we know what player 2's action is. \n",
        "      \n",
        "    Parameters: \n",
        "    opponent_action (int): 0, 1 or 2 (Rock, Paper or Scissors)\n",
        "  \n",
        "    Returns: \n",
        "    list of ints: [u(Rock), u(Paper), u(Scissors)] \n",
        "    \"\"\"\n",
        "\n",
        "    # WRITE YOUR CODE HERE\n",
        "    # You need to return the utility of playing Rock, Paper or Scissors, given what your opponent played\n",
        "    if opponent_action == 0:\n",
        "      return (0, 1, -1)\n",
        "    if opponent_action == 1:\n",
        "      return (-1, 0, 1)\n",
        "    if opponent_action == 2:\n",
        "      return [1,-1,0]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Tests. \n",
        "# The lines below test if the function works as it should\n",
        "# Each line checks if the function returns the correct action utilities, given the input \n",
        "assert np.array_equal(get_action_utilities(0), [0,1,-1])\n",
        "assert np.array_equal(get_action_utilities(1), [-1,0,1])\n",
        "assert np.array_equal(get_action_utilities(2), [1,-1,0])\n",
        "print(\"All tests passed\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLSntNMus7Ab"
      },
      "source": [
        "## Challenge - Write A Function That Gives Player 1's Regrets\n",
        "\n",
        "We need another helper function that gives us the regret for player 1 of not playing Rock, Paper or Scissors, given what player 1 and player 2 did. You will find it useful to use the get_action_utilities function that you just wrote.\n",
        "\n",
        "NOTE: You need to run the cell above to save the function in the notebook's memory, before you can use it in another cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsP0jYGBtWqN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "e7f243f6-5eb5-4a72-a917-eba5a13320ca"
      },
      "source": [
        "def get_regrets(my_action, opponent_action):\n",
        "  \"\"\"\n",
        "  Get the regrets for player 1 of not playing an action, \n",
        "  given what what both player's actions were. \n",
        "      \n",
        "    Parameters: \n",
        "    my_action (int) : 0, 1 or 2 (Rock, Paper or Scissors)\n",
        "    opponent_action (int): 0, 1 or 2 (Rock, Paper or Scissors)\n",
        "  \n",
        "    Returns: \n",
        "    list of ints: [regret(Rock), regret(Paper), regret(Scissors)] \n",
        "  \"\"\"\n",
        "\n",
        "  # WRITE YOUR CODE HERE\n",
        "  # You need to work out the regrets that you have based on what you and your opponent played\n",
        "  # For each action:\n",
        "  #   regret = utility of the action - utility of what you did play\n",
        "  # Use the function that you just wrote, get_action_utilities(), to help\n",
        "  if my_action == 0:\n",
        "    my_utility = (0, 1, -1)\n",
        "  if my_action == 1:\n",
        "    my_utility = (-1, 0, 1)\n",
        "  if my_action == 2:\n",
        "    my_utility = (1, -1, 0)\n",
        "  \n",
        "  my_utilities = np.array(my_utility)\n",
        "  his_utilities = np.array(get_action_utilities(opponent_action))\n",
        "  \n",
        "  print(my_utilities - his_utilities)\n",
        "  return \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Tests\n",
        "assert np.array_equal(get_regrets(0,0), [0,1,-1])\n",
        "assert np.array_equal(get_regrets(2,2), [1,-1,0])\n",
        "assert np.array_equal(get_regrets(0,1), [0,1,2])\n",
        "assert np.array_equal(get_regrets(1,0), [-1,0,-2])\n",
        "assert np.array_equal(get_regrets(2,0), [1,2,0])\n",
        "# These tests don't cover all cases. Feel free to add more, or if you are confident then move on\n",
        "print(\"All tests passed\")\n",
        "\n",
        "# In our example, player 1 played Rock against player 2's Paper.\n",
        "print(\"Our regret in the example is \", get_regrets(0,1))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 0]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-b7736bfbcf50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m#Tests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_regrets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_regrets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_regrets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdK4v60b0iYq"
      },
      "source": [
        "### Regret Matching \n",
        "One idea to improve our strategy would be to play the action that we most regretted last time. In our example above where we played Rock against our opponent's Paper, we most regret not playing Scissors, and so we will choose to play Scissors next time. That would work against a very dumb opponent who always plays Paper. But it will make us very predictable, and so very exploitable by a smart opponent. \n",
        "\n",
        "We can add some unpredictability by randomly selecting an action **proportional to positive regrets**. In our example above our regrets so far for not playing Rock, Paper, Scissors are 0, 1 and 2. In order to turn that into a strategy we need to write down our probability of playing each action. Probabilities have to sum to 1, so we can **normalise** the regrets by dividing each by the total regrets (0+1+2=3). \n",
        "\n",
        "This would give us a mixed strategy where the probabilty of playing Rock is $0$, the probability of playing Paper is $\\frac{1}{3}$ and the probability of playing Scissors is $\\frac{2}{3}$. That mixed strategy can be written conveniently as ($0$, $\\frac{1}{3}$, $\\frac{2}{3}$). These probabilites are also called **normalised positive regrets**. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPlf3G3WIez8"
      },
      "source": [
        "## Question: What Is The Weakness With This Strategy?\n",
        "\n",
        "Have a think about this and write your answer here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03_MapPjIRHM"
      },
      "source": [
        "## Multiple Rounds\n",
        "\n",
        "####Round 1 Summary:\n",
        "*   action_p1: Rock\n",
        "*   action_p2: Paper\n",
        "*   regrets_sum_p1: [0,1,2]\n",
        "*   new strategy_p1: [0,1/3,2/3]\n",
        "\n",
        "In round 2, we choose Paper. Let's assume that our opponent chooses Paper as well. What does this do to our regrets and strategy? \n",
        "\n",
        "We can work out our regrets using the function we just wrote. $\\text{get_regrets}(1,1)$ should return $[-1,0,1]$. We can add that to our regrets from round 1 to get **total cumulative regrets** of $[-1,1,3]$. We have a negative total regret of playing Rock - this means that we are happy that we did not play it, and so we do not want to play it in the future. In the same way as before our new strategy is determined by our **normalised positive regrets** (we ignore the negative regrets). This gives us a new strategy of $[0,1/4,3/4]$. \n",
        "\n",
        "####Round 2 Summary:\n",
        "*   action_p1: Paper\n",
        "*   action_p2: Paper\n",
        "*   regrets_sum_p1: [-1,1,3]\n",
        "*   new strategy_p1: [0,1/4,3/4]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWiIjkCh3SCW"
      },
      "source": [
        "## Challenge - Write A Function That Gives A Strategy Based on A Player's Total Regrets\n",
        "\n",
        "You need to find the **normalised positive regrets** given the total regrets. Here are some tips:\n",
        "\n",
        "*   Make sure that your function handles negative regrets correctly, and only considers positive regrets when calculating the strategy. \n",
        "*   The strategy should return normalised probabilities, so they need to sum to 1. \n",
        "*   If there are no positive regrets at all, then your function should return a uniform strategy.   \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yo1vk6H235hu"
      },
      "source": [
        "def get_strategy(regret_sum):\n",
        "  \"\"\"\n",
        "  Get a strategy given a regret_sum. \n",
        "  \n",
        "  The strategy is equivalent to the normalised positive regrets. \n",
        "\n",
        "  If there are no positive regrets, the uniform startegy [1/3,1/3,1/3] is returned\n",
        "\n",
        "    Parameters: \n",
        "    regret_sum (list of ints) : [cumulative regret(Rock), cumulative regret(Paper), cumulative regret(Scissors)]\n",
        "    \n",
        "    Returns: \n",
        "    list of floats: [p(Rock), p(Paper), p(Scissors)]\n",
        "    # Probabilities of playing each action \n",
        "  \"\"\"\n",
        "\n",
        "  # WRITE YOUR CODE HERE\n",
        "  # You need to convert the regret_sum, a list of numbers, into normalised positive regrets\n",
        "  # 1. Convert any negative regrets to zero\n",
        "  # 2. Normalise the array so that it adds to 1\n",
        "  # That will give you a strategy profile\n",
        "  # Take care of edge cases such as what happens if all the regrets are negative\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Your function should pass all of these tests\n",
        "assert np.array_equal(get_strategy([1,1,1]), [1/3,1/3,1/3])\n",
        "assert np.array_equal(get_strategy([0,1,2]), [0,1/3,2/3])\n",
        "assert np.array_equal(get_strategy([4,10,3]), [4/17,10/17,3/17])\n",
        "assert np.array_equal(get_strategy([0,1,0]), [0,1,0])\n",
        "assert np.array_equal(get_strategy([-1,1,3]), [0,1/4,3/4])\n",
        "assert np.array_equal(get_strategy([5,-2,2]), [5/7,0,2/7])\n",
        "assert np.array_equal(get_strategy([2,2,-8]), [1/2,1/2,0])\n",
        "assert np.array_equal(get_strategy([0,0,0]), [1/3,1/3,1/3])\n",
        "assert np.array_equal(get_strategy([-2,-3,-1]), [1/3,1/3,1/3])\n",
        "assert np.array_equal(get_strategy([-2,-3,0]), [1/3,1/3,1/3])\n",
        "print(\"Tests passed\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JCGJfoV82OR"
      },
      "source": [
        "## Challenge - Write a 1 Player Regret Matching Algorithm\n",
        "\n",
        "We now have most of the pieces to buid a fully fledged regret matching algorithm. We will start off doing this for 1 player against a fixed opponent. The 1 player algorithm will learn to exploit any weaknesses the opponent has, given enough time. For example, if the opponent plays Scissors most often, your algorithm will learn to always play Rock to take advantage of that. \n",
        "\n",
        "The algorithm should follow the same procedure as what we did manually for the first 2 rounds. Each round, you need to add to your cumulative regrets, then update your strategy based on those cumulative regrets.\n",
        "\n",
        "There's one more thing - we need to keep track of a strategy_sum, which is all the strategies we played at each iteration added together. The strategy sum will smooth out any fluctuations and should converge to a good strategy to play against the opponent, given enough time. \n",
        "\n",
        "We've given you the basic pseudocode below. It will help to use the functions that you have written so far. We also give you one extra function that will give you a random action given a player's strategy. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUsNO9R49qd_"
      },
      "source": [
        "def get_action(strategy):\n",
        "  \"\"\"\n",
        "  Returns an action given a strategy, based on the strategy probabilities.\n",
        "  The strategy passed in must be normalised - it must sum to 1. \n",
        "  \n",
        "    Parameters: \n",
        "    strategy (list of 3 floats) : [prob(Rock), prob(Paper), prob(Scissors)]\n",
        "    \n",
        "    Returns: \n",
        "    integer: action (0=Rock, 1=Paper, 2=Scissors)\n",
        "\n",
        "  \"\"\"\n",
        "  return np.random.choice([0,1,2], p=strategy)\n",
        " \n",
        "\n",
        "def train_p1(iterations, strategy_p2):\n",
        "  \"\"\"\n",
        "  Trains a player to play against a fixed opponent by regret matching. \n",
        "  \n",
        "  Updates player 1's strategy at each iteration, based on the the cumulative regrets\n",
        "\n",
        "  Returns a normalised sum of all strategies played in all iterations\n",
        "\n",
        "    Returns: \n",
        "    list of floats: [p(Rock), p(Paper), p(Scissors)]\n",
        "    (Probabilities of playing each action) \n",
        "  \"\"\"\n",
        "  # Initialise regrets_sum and and strategy_sum \n",
        "  regrets_sum_p1 = np.zeros(3)\n",
        "  strategy_sum = np.zeros(3)\n",
        "  \n",
        "  # WRITE YOUR CODE HERE\n",
        "  # loop \n",
        "    # Get players 1's strategy based on the regrets_sum\n",
        "    # Get player 1's action based on player 1's strategy\n",
        "    # Get player 2's action based on player 2's strategy (strategy_p2)\n",
        "    # Get player 1's regrets based on player 1's action and player 2's action\n",
        "    # Add player 1's regrets to the regrets_sum\n",
        "    # Add player 1's strategy to the strategy sum\n",
        "  \n",
        "  #return the normalised strategy_sum as the strategy solution\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# There are no tests for this function because there is randomness involved, \n",
        "# so that answers will be different each time\n",
        "\n",
        "# Your function should learn to exploit your opponent's weaknesses.\n",
        "# If your opponent plays one of the actions a lot more than the others, \n",
        "# then your algorithm should learn to play whatever action beats that\n",
        "\n",
        "# In this exmaple, player 2 is playing Rock most often\n",
        "# Your algorithm should learn to play Paper a lot to beat player 2 \n",
        "train_p1(iterations=2000, strategy_p2 = [0.4,0.3,0.3])\n",
        "\n",
        "\n",
        "# What happens if you set the opponent strategy to [1/3,1/3,1/3]?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DdAw3WJoQJD"
      },
      "source": [
        "## Q. What happens if you set the opponent's strategy to [1/3, 1/3, 1/3]? Why Does That Happen?\n",
        "\n",
        "Write your answer here.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TUyNREbpqNn"
      },
      "source": [
        "## The Weaknesses with Simple Regret Matching\n",
        "\n",
        "As you have seen, regret matching is good at finding weaknesses against a fixed opponent. But there are 2 big problems:\n",
        "\n",
        "### Weakness 1 - Exploitability\n",
        "During the course of play, the regret matching algorithm plays lots of strategies away from Nash Equilibrium. If an opponent can detect that, or if an opponent knows that you are playing regret matching, then the opponent can work out what your strategy is and exploit it. \n",
        "\n",
        "In the example we used at the start, our strategy for round 2 was $[0, 1/3, 2/3]$. If our opponent knows that then they can get a big advantage by playing Scissors next round. We have no chance of playing Rock and winning against Scissors with our strategy, but our opponent will win if we play Paper. Our strategy is exploitable. \n",
        "\n",
        "### Weakness 2 - Nash Equilibrium Opponent\n",
        "If the opponent is playing at Nash Equilibrium, in this case $[1/3, 1/3, 1/3]$, then our regret matching algorithm is overly influenced by chance fluctuations.  If you play against an opponent like this, there is no weakness to exploit. But by chance the opponent will probably play one action more than another, which the regret matching algorithm will pick up on and treat the same way as a weakness. This can result in very exploitable strategies. \n",
        "\n",
        "## Self Play\n",
        "Regret matching is more useful when used with self-play. It has been shown [1] that in a zero-sum 2 player simultaneous game, players who are both using regret minimisation strategies will converge to a Nash Equilibrium. Nash Equilibrium is a defensive solution concept - it cannot be exploited in a symmetric 2 player game.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xDJx5JXFZEg"
      },
      "source": [
        "## Challenge - Adapt the Train Algorithm to A Self-Play Algorithm \n",
        "\n",
        "We recommend that you begin by copy and pasting your code from the 1 player algorithm, and then adapting it so that player 2 updates their strategy the same way that player 1 does. You can add both players' strategies to the strategy_sum on each iteration. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wdsFPg9Fwa7"
      },
      "source": [
        "def train(iterations):\n",
        "  \"\"\"\n",
        "  Trains 2 players through regret matching to find a Nash equilibrium for \n",
        "  Rock Paper Scissors. \n",
        "  \n",
        "  Updates both players' startegies at each iteration, based on the the cumulative regrets\n",
        "\n",
        "  Returns a normalised sum of all strategies played in all iterations, across both players\n",
        "\n",
        "    Returns: \n",
        "    list of floats: [p(Rock), p(Paper), p(Scissors)]\n",
        "    # Probabilities of playing each action \n",
        "  \"\"\"\n",
        "\n",
        "  # Initialise regrets_sum and and strategy_sum \n",
        "  regrets_sum_p1 = np.zeros(3)\n",
        "  regrets_sum_p2 = np.zeros(3)\n",
        "  strategy_sum = np.zeros(3)\n",
        "\n",
        "  # WRITE YOUR CODE HERE\n",
        "  # Very similar to the 1 player algorithm\n",
        "  # But this time treat player 2 the same way as player 1, tracking regrets and updating the strategy\n",
        "\n",
        "  # Return the sum of both players strategies, normalised\n",
        "\n",
        "\n",
        "\n",
        "train(iterations=1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvJUsoeNLjYp"
      },
      "source": [
        "## Question - What are the advantages and disadvantages of playing at Nash Equilibrium versus trying to exploit your opponent's weaknesses? What would happen if you were playing against someone where you knew (or could work out) what their strategy was? What would happen if you were playing against someone who knew (or could work out) what your strategy was?\n",
        "\n",
        "Write your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-CpL0AStSBN"
      },
      "source": [
        "## Extra Challenge - Can You Visualise the Learning Process Using a Plot?\n",
        "\n",
        "How could you best visualise what is going on during the regret matching algorithm?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBcNGKmSuM5y"
      },
      "source": [
        "## WRITE YOUR CODE HERE\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whg23hjrmzxh"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "## Follow Up Project - Can You Generalise Your Code to Any Game?\n",
        "\n",
        "Can you adapt your code to work for more general games, beyond Rock Paper Scissors? For example with different payoff matrices and more possible actions per player?\n",
        "\n",
        "Also, think about what the limits of regret matching are - what games will it not work for and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfP0dwY4bnDv"
      },
      "source": [
        "## Ackowledgements\n",
        "\n",
        "The examples and code in this notebook were adapted from [3].\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "namC5pi2bXcF"
      },
      "source": [
        "## References\n",
        "\n",
        "[1] A Simple Adaptive Procedure Leading to\n",
        "Correlated Equilibrium, Hart and Mas-Colell, 2000\n",
        "https://wwwf.imperial.ac.uk/~dturaev/Hart0.pdf\n",
        "\n",
        "[2] Regret Minimization in Games with Incomplete\n",
        "Information, Zinkevich et al, 2007\n",
        "https://poker.cs.ualberta.ca/publications/NIPS07-cfr.pdf\n",
        "\n",
        "[3] An Introduction to Counterfactual Regret Minimization, Neller, 2013\n",
        "http://modelai.gettysburg.edu/2013/cfr/cfr.pdf\n",
        "\n"
      ]
    }
  ]
}